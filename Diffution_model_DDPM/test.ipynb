{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737ba58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "\n",
    "\n",
    "# --- 1. DEFINE THE SAMPLING FUNCTION FOR A SINGLE TIMESTEP ---\n",
    "\n",
    "@torch.no_grad() # Crucial: we are not training, so we don't need to calculate gradients\n",
    "def sample_timestep(x, t, model):\n",
    "    \"\"\"\n",
    "    Calls the model to predict the noise and uses it to denoise the image for one timestep.\n",
    "    This is the core of Algorithm 2 from the paper.\n",
    "    \"\"\"\n",
    "    betas_t = get_index_from_list(betas, t, x.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = get_index_from_list(\n",
    "        sqrt_one_minus_alphas_cumprod, t, x.shape\n",
    "    )\n",
    "    sqrt_recip_alphas_t = get_index_from_list(sqrt_recip_alphas, t, x.shape)\n",
    "    \n",
    "    # This is the formula from Algorithm 2, Step 4\n",
    "    # Call model (current image - noise prediction)\n",
    "    model_mean = sqrt_recip_alphas_t * (\n",
    "        x - betas_t * model(x, t) / sqrt_one_minus_alphas_cumprod_t\n",
    "    )\n",
    "    posterior_variance_t = get_index_from_list(posterior_variance, t, x.shape)\n",
    "    \n",
    "    if t.all() == 0:\n",
    "        # As stated in the paper, at the last step we don't add noise\n",
    "        return model_mean\n",
    "    else:\n",
    "        # Add random noise\n",
    "        noise = torch.randn_like(x)\n",
    "        return model_mean + torch.sqrt(posterior_variance_t) * noise\n",
    "\n",
    "# --- 2. DEFINE THE FULL SAMPLING LOOP ---\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_plot_images(model, num_images=16):\n",
    "    print(\"Generating new images...\")\n",
    "    # Start with pure random noise (our blank canvas)\n",
    "    img = torch.randn((num_images, 3, IMG_SIZE, IMG_SIZE), device=DEVICE)\n",
    "    \n",
    "    # Use tqdm for a nice progress bar\n",
    "    for i in tqdm(reversed(range(0, TIMESTEPS)), desc='Sampling loop', total=TIMESTEPS):\n",
    "        # Create a tensor of the current timestep for all images in the batch\n",
    "        t = torch.full((num_images,), i, device=DEVICE, dtype=torch.long)\n",
    "        # Perform one denoising step\n",
    "        img = sample_timestep(img, t, model)\n",
    "\n",
    "    # Display the final generated images\n",
    "    show_images(img, \"Generated Images\")\n",
    "\n",
    "\n",
    "# --- 3. LOAD YOUR TRAINED MODEL AND RUN SAMPLING ---\n",
    "\n",
    "# Create a new instance of the model (make sure architecture is the same)\n",
    "model = SimpleUnet().to(DEVICE)\n",
    "\n",
    "# Load the saved weights\n",
    "model_path = \"ddpm_cifar10_10_epochs.pth\"\n",
    "model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Run the sampling\n",
    "sample_plot_images(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Diffution_model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
